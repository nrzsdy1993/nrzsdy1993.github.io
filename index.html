<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>여리 블로그</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="여리 블로그"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="여리 블로그"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="여리 블로그"><meta property="og:url" content="https://nrzsdy1993.github.io/"><meta property="og:site_name" content="여리 블로그"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://nrzsdy1993.github.io/img/og_image.png"><meta property="article:author" content="김덕열"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nrzsdy1993.github.io"},"headline":"여리 블로그","image":["https://nrzsdy1993.github.io/img/og_image.png"],"author":{"@type":"Person","name":"김덕열"},"publisher":{"@type":"Organization","name":"여리 블로그","logo":{"@type":"ImageObject","url":null}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">여리 블로그</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-22T03:30:56.310Z" title="2021. 10. 22. 오후 12:30:56">2021-10-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-10-22T03:37:20.019Z" title="2021. 10. 22. 오후 12:37:20">2021-10-22</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></span><span class="level-item">4 minutes read (About 529 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/22/step02_1_MNIST/">MNIST dataset</a></h1><div class="content"><ul>
<li>관련 주소<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nrzsdy1993/MLP">링크</a></li>
</ul>
</li>
</ul>
<h2 id="torch-GPU-설정"><a href="#torch-GPU-설정" class="headerlink" title="torch GPU 설정"></a>torch GPU 설정</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="MNIST-데이터-셋"><a href="#MNIST-데이터-셋" class="headerlink" title="MNIST 데이터 셋"></a>MNIST 데이터 셋</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">PATH_DATA = <span class="string">&quot;./data&quot;</span></span><br><span class="line">train_data = datasets.MNIST(PATH_DATA, train = <span class="literal">True</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz



  0%|          | 0/9912422 [00:00&lt;?, ?it/s]


Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz



  0%|          | 0/28881 [00:00&lt;?, ?it/s]


Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz



  0%|          | 0/1648877 [00:00&lt;?, ?it/s]


Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz



  0%|          | 0/4542 [00:00&lt;?, ?it/s]


Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw



/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
</code></pre>
<ul>
<li>extract the input data and target labels</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train, y_train = train_data.data, train_data.targets</span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([60000, 28, 28])
torch.Size([60000])
</code></pre>
<ul>
<li>Load the MNIST test dataset</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val_data = datasets.MNIST(PATH_DATA, train = <span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line">X_val, y_val = val_data.data, val_data.targets</span><br><span class="line"><span class="built_in">print</span>(X_val.shape)</span><br><span class="line"><span class="built_in">print</span>(y_val.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10000, 28, 28])
torch.Size([10000])
</code></pre>
<ul>
<li>Add a new dimension to the tensors</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(X_train.shape) == <span class="number">3</span>:</span><br><span class="line">  X_train = X_train.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(X_val.shape) == <span class="number">3</span>:</span><br><span class="line">  X_val = X_val.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X_val.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([60000, 1, 28, 28])
torch.Size([10000, 1, 28, 28])
</code></pre>
<h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span>(<span class="params">img</span>):</span></span><br><span class="line">  <span class="comment"># convert tensor to numpy array</span></span><br><span class="line">  np_img = img.numpy()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Convert to H * W * C shape</span></span><br><span class="line">  np_img_tr = np.transpose(np_img, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  plt.imshow(np_img_tr, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_grid = utils.make_grid(X_train[:<span class="number">20</span>], nrow=<span class="number">4</span>, padding=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(X_grid.shape)</span><br><span class="line"></span><br><span class="line">show(X_grid)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 152, 122])
</code></pre>
<p><img src="/images/output_13_1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-22T02:37:34.467Z" title="2021. 10. 22. 오전 11:37:34">2021-10-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-10-22T03:47:59.387Z" title="2021. 10. 22. 오후 12:47:59">2021-10-22</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></span><span class="level-item">4 minutes read (About 635 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/22/step01_intro_pytorch/">PyTorch</a></h1><div class="content"><ul>
<li>관련 주소<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nrzsdy1993/PyTorch">링크</a></li>
</ul>
</li>
</ul>
<h2 id="Numpy-and-PyTorch-Tensors-Comparison"><a href="#Numpy-and-PyTorch-Tensors-Comparison" class="headerlink" title="Numpy and PyTorch Tensors, Comparison"></a>Numpy and PyTorch Tensors, Comparison</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>1.19.5
1.8.1+cu101
</code></pre>
<ul>
<li>Create an array of ones</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.ones(<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.ones(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[1. 1. 1. 1. 1.]
tensor([1., 1., 1., 1., 1.])
</code></pre>
<ul>
<li>Create an array of zeros</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.zeros(<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.zeros(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[0. 0. 0. 0. 0.]
tensor([0., 0., 0., 0., 0.])
</code></pre>
<ul>
<li>Create a random array</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.random.rand(<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[0.64360299 0.39417631 0.79543956 0.39073155 0.85939403]
tensor([0.8248, 0.5975, 0.1073, 0.1417, 0.2082])
</code></pre>
<ul>
<li>Create an array from given values</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.array(<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>5
tensor(5)
</code></pre>
<ul>
<li>Get an array shape</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = np.zeros((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(temp.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(2, 5)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temp = torch.zeros((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(temp.shape)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([2, 5])
</code></pre>
<h2 id="Tensor-Data-Type"><a href="#Tensor-Data-Type" class="headerlink" title="Tensor Data Type"></a>Tensor Data Type</h2><ul>
<li>Define a tensor with a default data type</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1.],
        [1., 1.]])
torch.float32
</code></pre>
<ul>
<li>Specify the data type when defining a tensor</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, dtype = torch.int8)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 1],
        [1, 1]], dtype=torch.int8)
torch.int8
</code></pre>
<h2 id="Changing-the-tensor’s-data-type"><a href="#Changing-the-tensor’s-data-type" class="headerlink" title="Changing the tensor’s data type"></a>Changing the tensor’s data type</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, dtype=torch.uint8)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>torch.uint8
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>torch.float32
</code></pre>
<h2 id="Converting-tensors-into-NumPy-arrays"><a href="#Converting-tensors-into-NumPy-arrays" class="headerlink" title="Converting tensors into NumPy arrays"></a>Converting tensors into NumPy arrays</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0.7788, 0.0314],
        [0.4117, 0.6491]])
torch.float32
</code></pre>
<ul>
<li>Convert the tensor into a NumPy array</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x.numpy()</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>[[0.77875483 0.0314005 ]
 [0.41166532 0.64910847]]
float32
</code></pre>
<h2 id="Arithmetic-Operation"><a href="#Arithmetic-Operation" class="headerlink" title="Arithmetic Operation"></a>Arithmetic Operation</h2><ul>
<li>텐서를 활용하여 사칙 연산을 수행합니다.</li>
</ul>
<h3 id="Scalar"><a href="#Scalar" class="headerlink" title="Scalar"></a>Scalar</h3><ul>
<li>값 하나의 상수값을 의미한다. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar_1 = torch.tensor([<span class="number">1.</span>])</span><br><span class="line">scalar_2 = torch.tensor([<span class="number">5.</span>])</span><br><span class="line"></span><br><span class="line">add = scalar_1 + scalar_2</span><br><span class="line">torch_add = torch.add(scalar_1, scalar_2)</span><br><span class="line"><span class="built_in">print</span>(add)</span><br><span class="line"><span class="built_in">print</span>(torch_add)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([6.])
tensor([6.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sub = scalar_1 - scalar_2</span><br><span class="line">torch_sub = torch.sub(scalar_1, scalar_2)</span><br><span class="line"><span class="built_in">print</span>(sub)</span><br><span class="line"><span class="built_in">print</span>(torch_sub)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-4.])
tensor([-4.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mul = scalar_1 * scalar_2</span><br><span class="line">torch_mul = torch.mul(scalar_1, scalar_2)</span><br><span class="line"><span class="built_in">print</span>(mul)</span><br><span class="line"><span class="built_in">print</span>(torch_mul)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.])
tensor([5.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">div = scalar_1 / scalar_2</span><br><span class="line">torch_div = torch.div(scalar_1, scalar_2)</span><br><span class="line"><span class="built_in">print</span>(div)</span><br><span class="line"><span class="built_in">print</span>(torch_div)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([0.2000])
tensor([0.2000])
</code></pre>
<h3 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h3><ul>
<li>하나의 값을 표현할 때 2개 이상의 수치로 표현한 것</li>
<li>사칙 연산의 문법은 Scalar와 동일하게 적용하기 때문에 생략합니다. </li>
<li>torch.dot에 대해 설명하면 다음과 같습니다. <ul>
<li>공식: (1 * 4) + (2 * 5) + (3 * 6) = 4 + 10 + 18 = 32</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector_1 = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">vector_2 = torch.tensor([<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>])</span><br><span class="line">torch.dot(vector_1, vector_2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(32.)
</code></pre>
<h3 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h3><ul>
<li>2개 이상의 벡터 값을 통합해 구성된 값</li>
<li>선형 대수의 기본 단위임. </li>
<li>사칙연산도 기존 Scalar와 동일하기 때문에 생략한다.</li>
<li>행렬 곱 연산은 torch.matmul을 활용할 수 있다. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">matrix_1 = torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]])</span><br><span class="line">matrix_2 = torch.tensor([[<span class="number">5.</span>, <span class="number">6.</span>], [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.matmul(matrix_1, matrix_2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-- 행렬 곱 연산 --&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span> * <span class="number">5</span> + <span class="number">2</span> * <span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span> * <span class="number">6</span> + <span class="number">2</span> * <span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">3</span> * <span class="number">5</span> + <span class="number">4</span> * <span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="number">3</span> * <span class="number">6</span> + <span class="number">4</span> * <span class="number">8</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[19., 22.],
        [43., 50.]])
-- 행렬 곱 연산 --
19
22
43
50
</code></pre>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><ul>
<li>행렬이 2차원의 배열이라 한다면, 텐서는 2차원 이상의 배열</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor_1 = torch.tensor([[[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]], [[<span class="number">5.</span>, <span class="number">6.</span>], [<span class="number">7.</span>, <span class="number">8.</span>]]])</span><br><span class="line">tensor_2 = torch.tensor([[[<span class="number">9.</span>, <span class="number">10.</span>], [<span class="number">11.</span>, <span class="number">12.</span>]], [[<span class="number">13.</span>, <span class="number">14.</span>], [<span class="number">15.</span>, <span class="number">16.</span>]]])</span><br><span class="line"></span><br><span class="line">torch.matmul(tensor_1, tensor_2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 31.,  34.],
         [ 71.,  78.]],

        [[155., 166.],
         [211., 226.]]])
</code></pre>
<h2 id="Converting-NumPy-arrays-into-tensors"><a href="#Converting-NumPy-arrays-into-tensors" class="headerlink" title="Converting NumPy arrays into tensors"></a>Converting NumPy arrays into tensors</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.zeros((<span class="number">2</span>, <span class="number">2</span>), dtype=np.float32)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>[[0. 0.]
 [0. 0.]]
float32
</code></pre>
<ul>
<li>Convert the numpy array into PyTorch tensor </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.from_numpy(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.dtype)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0.],
        [0., 0.]])
torch.float32
</code></pre>
<h2 id="Moving-between-devices"><a href="#Moving-between-devices" class="headerlink" title="Moving between devices"></a>Moving between devices</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.5</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1.5000, 2.0000])
cpu
</code></pre>
<h2 id="CUDA-Device"><a href="#CUDA-Device" class="headerlink" title="CUDA Device"></a>CUDA Device</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = x.to(device)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1.5000, 2.0000], device=&#39;cuda:0&#39;)
cuda:0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">x = x.to(device)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1.5000, 2.0000])
cpu
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, device=device)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1.],
        [1., 1.]], device=&#39;cuda:0&#39;)
</code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-22T00:18:50.704Z" title="2021. 10. 22. 오전 9:18:50">2021-10-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-10-22T03:40:10.071Z" title="2021. 10. 22. 오후 12:40:10">2021-10-22</time></span><span class="level-item"><a class="link-muted" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></span><span class="level-item">14 minutes read (About 2044 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/22/step02_2_MLP/">MLP</a></h1><div class="content"><ul>
<li>관련 주소<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nrzsdy1993/MLP">링크</a></li>
</ul>
</li>
</ul>
<h2 id="MLP-모델이란"><a href="#MLP-모델이란" class="headerlink" title="MLP 모델이란?"></a>MLP 모델이란?</h2><ul>
<li>퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. </li>
<li>간단히 비교하면, <ul>
<li>퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. </li>
<li>MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. </li>
</ul>
</li>
</ul>
<h2 id="MLP-모델을-활용한-MNIST-모델-개발"><a href="#MLP-모델을-활용한-MNIST-모델-개발" class="headerlink" title="MLP 모델을 활용한 MNIST 모델 개발"></a>MLP 모델을 활용한 MNIST 모델 개발</h2><ul>
<li>MLP 모델 순서대로 코드를 구현합니다. </li>
</ul>
<h3 id="Step-1-모듈-불러오기"><a href="#Step-1-모듈-불러오기" class="headerlink" title="Step 1. 모듈 불러오기"></a>Step 1. 모듈 불러오기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br></pre></td></tr></table></figure>

<h3 id="Step-2-딥러닝-모델-설계-시-필요한-장비-세팅"><a href="#Step-2-딥러닝-모델-설계-시-필요한-장비-세팅" class="headerlink" title="Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅"></a>Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  DEVICE = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  DEVICE = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch Version:&quot;</span>, torch.__version__, <span class="string">&#x27; Device:&#x27;</span>, DEVICE)</span><br></pre></td></tr></table></figure>

<pre><code>PyTorch Version: 1.9.0+cu102  Device: cuda
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment"># 데이터가 32개로 구성되어 있음. </span></span><br><span class="line">EPOCHS = <span class="number">10</span> <span class="comment"># 전체 데이터 셋을 10번 반복해 학습함.</span></span><br></pre></td></tr></table></figure>

<h3 id="Step-3-데이터-다운로드"><a href="#Step-3-데이터-다운로드" class="headerlink" title="Step 3. 데이터 다운로드"></a>Step 3. 데이터 다운로드</h3><ul>
<li>torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다.</li>
<li>ToTensor() 활용하여 데이터셋을 tensor 형태로 변환</li>
<li>한 픽셀은 0<del>255 범위의 스칼라 값으로 구성, 이를 0</del>1 범위에서 정규화 과정 진행</li>
<li>DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미<ul>
<li>Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = datasets.MNIST(root = <span class="string">&quot;../data/MNIST&quot;</span>, </span><br><span class="line">                               train = <span class="literal">True</span>, </span><br><span class="line">                               download = <span class="literal">True</span>, </span><br><span class="line">                               transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root = <span class="string">&quot;../data/MNIST&quot;</span>, </span><br><span class="line">                              train = <span class="literal">False</span>, </span><br><span class="line">                              transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset = train_dataset, </span><br><span class="line">                                           batch_size = BATCH_SIZE, </span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset = test_dataset, </span><br><span class="line">                                           batch_size = BATCH_SIZE, </span><br><span class="line">                                           shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz



HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value=&#39;&#39;)))



Extracting ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 503: Service Unavailable

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz



HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value=&#39;&#39;)))



Extracting ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 503: Service Unavailable

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz



HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value=&#39;&#39;)))



Extracting ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 503: Service Unavailable

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz



HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value=&#39;&#39;)))



Extracting ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw



/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
</code></pre>
<h3 id="step-4-데이터-확인-및-시각화"><a href="#step-4-데이터-확인-및-시각화" class="headerlink" title="step 4. 데이터 확인 및 시각화"></a>step 4. 데이터 확인 및 시각화</h3><ul>
<li>데이터를 확인하고 시각화를 진행합니다. </li>
<li>32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (X_train, y_train) <span class="keyword">in</span> train_loader:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;X_train:&#x27;</span>, X_train.size(), <span class="string">&#x27;type:&#x27;</span>, X_train.<span class="built_in">type</span>())</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;y_train:&#x27;</span>, y_train.size(), <span class="string">&#x27;type:&#x27;</span>, y_train.<span class="built_in">type</span>())</span><br><span class="line">  <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor
y_train: torch.Size([32]) type: torch.LongTensor
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pltsize = <span class="number">1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span> * pltsize, pltsize))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i + <span class="number">1</span>)</span><br><span class="line">  plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">  plt.imshow(X_train[i, :, :, :].numpy().reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&quot;gray_r&quot;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Class: &#x27;</span> + <span class="built_in">str</span>(y_train[i].item()))</span><br></pre></td></tr></table></figure>


<p><img src="/images/output_11_0.png"></p>
<h3 id="step-5-MLP-모델-설계"><a href="#step-5-MLP-모델-설계" class="headerlink" title="step 5. MLP 모델 설계"></a>step 5. MLP 모델 설계</h3><ul>
<li>torch 모듈을 이용해 MLP를 설계합니다. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Forward Propagation 정의</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">    self.fc1 = nn.Linear(<span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>, <span class="number">512</span>) <span class="comment"># (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정</span></span><br><span class="line">    self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>) <span class="comment"># Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정</span></span><br><span class="line">    self.fc3 = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># Input 노드수는 256, Output 노드수는 10개로 지정</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = x.view(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>) <span class="comment"># 1차원으로 펼친 이미지 데이터 통과</span></span><br><span class="line">    x = self.fc1(x)</span><br><span class="line">    x = F.sigmoid(x)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = F.sigmoid(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = F.log_softmax(x, dim = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="step-6-옵티마이저-목적-함수-설정"><a href="#step-6-옵티마이저-목적-함수-설정" class="headerlink" title="step 6. 옵티마이저 목적 함수 설정"></a>step 6. 옵티마이저 목적 함수 설정</h3><ul>
<li>Back Propagation 설정 위한 목적 함수 설정 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Net().to(DEVICE)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># output 값과 원-핫 인코딩 값과의 Loss </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (fc1): Linear(in_features=784, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=10, bias=True)
)
</code></pre>
<h3 id="step-7-MLP-모델-학습"><a href="#step-7-MLP-모델-학습" class="headerlink" title="step 7. MLP 모델 학습"></a>step 7. MLP 모델 학습</h3><ul>
<li>MLP 모델을 학습 상태로 지정하는 코드를 구현 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, train_loader, optimizer, log_interval</span>):</span></span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> batch_idx, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader): <span class="comment"># 모형 학습</span></span><br><span class="line">    image = image.to(DEVICE)</span><br><span class="line">    label = label.to(DEVICE)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># Optimizer의 Gradient 초기화</span></span><br><span class="line">    output = model(image)</span><br><span class="line">    loss = criterion(output, label)</span><br><span class="line">    loss.backward() <span class="comment"># back propagation 계산</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125;(&#123;:.0f&#125;%)]\tTrain Loass: &#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(Epoch, batch_idx * <span class="built_in">len</span>(image), </span><br><span class="line">                                                                           <span class="built_in">len</span>(train_loader.dataset), <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br></pre></td></tr></table></figure>

<h3 id="step-8-검증-데이터-확인-함수"><a href="#step-8-검증-데이터-확인-함수" class="headerlink" title="step 8. 검증 데이터 확인 함수"></a>step 8. 검증 데이터 확인 함수</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">model, test_loader</span>):</span></span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br><span class="line">  test_loss = <span class="number">0</span></span><br><span class="line">  correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> test_loader:</span><br><span class="line">      image = image.to(DEVICE)</span><br><span class="line">      label = label.to(DEVICE)</span><br><span class="line">      output = model(image)</span><br><span class="line">      test_loss += criterion(output, label).item()</span><br><span class="line">      prediction = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim = <span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">      correct += prediction.eq(label.view_as(prediction)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">  test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">  test_accuracy = <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">  <span class="keyword">return</span> test_loss, test_accuracy</span><br></pre></td></tr></table></figure>

<ul>
<li>모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어</li>
</ul>
<h3 id="step-9-MLP-학습-실행"><a href="#step-9-MLP-학습-실행" class="headerlink" title="step 9. MLP 학습 실행"></a>step 9. MLP 학습 실행</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> Epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, EPOCHS + <span class="number">1</span>):</span><br><span class="line">  train(model, train_loader, optimizer, log_interval=<span class="number">200</span>)</span><br><span class="line">  test_loss, test_accuracy = evaluate(model, test_loader)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;\n[EPOCH: &#123;&#125;], \tTest Loss: &#123;:.4f&#125;, \tTest Accuracy: &#123;:.2f&#125; %\n&quot;</span>.<span class="built_in">format</span>(Epoch, test_loss, test_accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;)


Train Epoch: 1 [0/60000(0%)]    Train Loass: 2.343919
Train Epoch: 1 [6400/60000(11%)]    Train Loass: 2.304868
Train Epoch: 1 [12800/60000(21%)]    Train Loass: 2.276658
Train Epoch: 1 [19200/60000(32%)]    Train Loass: 2.320047
Train Epoch: 1 [25600/60000(43%)]    Train Loass: 2.309569
Train Epoch: 1 [32000/60000(53%)]    Train Loass: 2.293312
Train Epoch: 1 [38400/60000(64%)]    Train Loass: 2.264061
Train Epoch: 1 [44800/60000(75%)]    Train Loass: 2.311200
Train Epoch: 1 [51200/60000(85%)]    Train Loass: 2.261489
Train Epoch: 1 [57600/60000(96%)]    Train Loass: 2.260837

[EPOCH: 1],     Test Loss: 0.0698,     Test Accuracy: 36.43 %

Train Epoch: 2 [0/60000(0%)]    Train Loass: 2.233048
Train Epoch: 2 [6400/60000(11%)]    Train Loass: 2.196863
Train Epoch: 2 [12800/60000(21%)]    Train Loass: 2.173305
Train Epoch: 2 [19200/60000(32%)]    Train Loass: 2.120425
Train Epoch: 2 [25600/60000(43%)]    Train Loass: 2.023674
Train Epoch: 2 [32000/60000(53%)]    Train Loass: 1.936951
Train Epoch: 2 [38400/60000(64%)]    Train Loass: 1.738194
Train Epoch: 2 [44800/60000(75%)]    Train Loass: 1.493235
Train Epoch: 2 [51200/60000(85%)]    Train Loass: 1.547187
Train Epoch: 2 [57600/60000(96%)]    Train Loass: 1.400939

[EPOCH: 2],     Test Loss: 0.0402,     Test Accuracy: 60.40 %

Train Epoch: 3 [0/60000(0%)]    Train Loass: 1.429194
Train Epoch: 3 [6400/60000(11%)]    Train Loass: 1.178954
Train Epoch: 3 [12800/60000(21%)]    Train Loass: 0.970049
Train Epoch: 3 [19200/60000(32%)]    Train Loass: 1.105888
Train Epoch: 3 [25600/60000(43%)]    Train Loass: 0.926736
Train Epoch: 3 [32000/60000(53%)]    Train Loass: 0.794726
Train Epoch: 3 [38400/60000(64%)]    Train Loass: 0.828843
Train Epoch: 3 [44800/60000(75%)]    Train Loass: 0.872613
Train Epoch: 3 [51200/60000(85%)]    Train Loass: 0.713790
Train Epoch: 3 [57600/60000(96%)]    Train Loass: 0.464628

[EPOCH: 3],     Test Loss: 0.0243,     Test Accuracy: 76.67 %

Train Epoch: 4 [0/60000(0%)]    Train Loass: 0.507696
Train Epoch: 4 [6400/60000(11%)]    Train Loass: 0.635314
Train Epoch: 4 [12800/60000(21%)]    Train Loass: 0.636553
Train Epoch: 4 [19200/60000(32%)]    Train Loass: 0.875113
Train Epoch: 4 [25600/60000(43%)]    Train Loass: 0.558601
Train Epoch: 4 [32000/60000(53%)]    Train Loass: 0.462972
Train Epoch: 4 [38400/60000(64%)]    Train Loass: 0.630431
Train Epoch: 4 [44800/60000(75%)]    Train Loass: 0.428842
Train Epoch: 4 [51200/60000(85%)]    Train Loass: 0.826000
Train Epoch: 4 [57600/60000(96%)]    Train Loass: 0.508183

[EPOCH: 4],     Test Loss: 0.0182,     Test Accuracy: 83.32 %

Train Epoch: 5 [0/60000(0%)]    Train Loass: 0.576632
Train Epoch: 5 [6400/60000(11%)]    Train Loass: 0.437238
Train Epoch: 5 [12800/60000(21%)]    Train Loass: 0.942570
Train Epoch: 5 [19200/60000(32%)]    Train Loass: 0.412996
Train Epoch: 5 [25600/60000(43%)]    Train Loass: 0.362784
Train Epoch: 5 [32000/60000(53%)]    Train Loass: 0.677201
Train Epoch: 5 [38400/60000(64%)]    Train Loass: 0.706629
Train Epoch: 5 [44800/60000(75%)]    Train Loass: 0.587188
Train Epoch: 5 [51200/60000(85%)]    Train Loass: 0.584579
Train Epoch: 5 [57600/60000(96%)]    Train Loass: 0.602702

[EPOCH: 5],     Test Loss: 0.0147,     Test Accuracy: 86.63 %

Train Epoch: 6 [0/60000(0%)]    Train Loass: 0.388631
Train Epoch: 6 [6400/60000(11%)]    Train Loass: 0.571896
Train Epoch: 6 [12800/60000(21%)]    Train Loass: 0.287511
Train Epoch: 6 [19200/60000(32%)]    Train Loass: 0.394190
Train Epoch: 6 [25600/60000(43%)]    Train Loass: 0.264939
Train Epoch: 6 [32000/60000(53%)]    Train Loass: 0.495090
Train Epoch: 6 [38400/60000(64%)]    Train Loass: 0.274051
Train Epoch: 6 [44800/60000(75%)]    Train Loass: 0.299830
Train Epoch: 6 [51200/60000(85%)]    Train Loass: 0.450571
Train Epoch: 6 [57600/60000(96%)]    Train Loass: 0.257513

[EPOCH: 6],     Test Loss: 0.0129,     Test Accuracy: 87.99 %

Train Epoch: 7 [0/60000(0%)]    Train Loass: 0.377794
Train Epoch: 7 [6400/60000(11%)]    Train Loass: 0.630070
Train Epoch: 7 [12800/60000(21%)]    Train Loass: 0.320578
Train Epoch: 7 [19200/60000(32%)]    Train Loass: 0.658281
Train Epoch: 7 [25600/60000(43%)]    Train Loass: 0.643368
Train Epoch: 7 [32000/60000(53%)]    Train Loass: 0.420115
Train Epoch: 7 [38400/60000(64%)]    Train Loass: 0.687097
Train Epoch: 7 [44800/60000(75%)]    Train Loass: 0.430246
Train Epoch: 7 [51200/60000(85%)]    Train Loass: 0.343727
Train Epoch: 7 [57600/60000(96%)]    Train Loass: 0.577327

[EPOCH: 7],     Test Loss: 0.0120,     Test Accuracy: 88.78 %

Train Epoch: 8 [0/60000(0%)]    Train Loass: 0.530810
Train Epoch: 8 [6400/60000(11%)]    Train Loass: 0.278406
Train Epoch: 8 [12800/60000(21%)]    Train Loass: 0.140812
Train Epoch: 8 [19200/60000(32%)]    Train Loass: 0.471123
Train Epoch: 8 [25600/60000(43%)]    Train Loass: 0.415495
Train Epoch: 8 [32000/60000(53%)]    Train Loass: 0.534980
Train Epoch: 8 [38400/60000(64%)]    Train Loass: 0.318894
Train Epoch: 8 [44800/60000(75%)]    Train Loass: 0.444783
Train Epoch: 8 [51200/60000(85%)]    Train Loass: 0.211995
Train Epoch: 8 [57600/60000(96%)]    Train Loass: 0.358874

[EPOCH: 8],     Test Loss: 0.0113,     Test Accuracy: 89.73 %

Train Epoch: 9 [0/60000(0%)]    Train Loass: 0.423838
Train Epoch: 9 [6400/60000(11%)]    Train Loass: 0.225967
Train Epoch: 9 [12800/60000(21%)]    Train Loass: 0.348597
Train Epoch: 9 [19200/60000(32%)]    Train Loass: 0.397753
Train Epoch: 9 [25600/60000(43%)]    Train Loass: 0.199611
Train Epoch: 9 [32000/60000(53%)]    Train Loass: 0.269096
Train Epoch: 9 [38400/60000(64%)]    Train Loass: 0.311267
Train Epoch: 9 [44800/60000(75%)]    Train Loass: 0.575633
Train Epoch: 9 [51200/60000(85%)]    Train Loass: 0.246814
Train Epoch: 9 [57600/60000(96%)]    Train Loass: 0.272062

[EPOCH: 9],     Test Loss: 0.0108,     Test Accuracy: 89.94 %

Train Epoch: 10 [0/60000(0%)]    Train Loass: 0.399807
Train Epoch: 10 [6400/60000(11%)]    Train Loass: 0.282056
Train Epoch: 10 [12800/60000(21%)]    Train Loass: 0.270553
Train Epoch: 10 [19200/60000(32%)]    Train Loass: 0.605585
Train Epoch: 10 [25600/60000(43%)]    Train Loass: 0.333442
Train Epoch: 10 [32000/60000(53%)]    Train Loass: 0.395121
Train Epoch: 10 [38400/60000(64%)]    Train Loass: 0.676208
Train Epoch: 10 [44800/60000(75%)]    Train Loass: 0.170694
Train Epoch: 10 [51200/60000(85%)]    Train Loass: 0.160667
Train Epoch: 10 [57600/60000(96%)]    Train Loass: 0.381771

[EPOCH: 10],     Test Loss: 0.0105,     Test Accuracy: 90.28 %
</code></pre>
<ul>
<li><p>train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함.</p>
</li>
<li><p>학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄. </p>
</li>
<li></li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="여리"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">여리</p><p class="is-size-6 is-block">Desperate triumphs over luck</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>South Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="level-start"><span class="level-item">딥러닝</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-22T03:30:56.310Z">2021-10-22</time></p><p class="title"><a href="/2021/10/22/step02_1_MNIST/">MNIST dataset</a></p><p class="categories"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-22T02:37:34.467Z">2021-10-22</time></p><p class="title"><a href="/2021/10/22/step01_intro_pytorch/">PyTorch</a></p><p class="categories"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-22T00:18:50.704Z">2021-10-22</time></p><p class="title"><a href="/2021/10/22/step02_2_MLP/">MLP</a></p><p class="categories"><a href="/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CUDA-DEVICE/"><span class="tag">CUDA_DEVICE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLP/"><span class="tag">MLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MNIST/"><span class="tag">MNIST</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PYTORCH/"><span class="tag">PYTORCH</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TENSOR/"><span class="tag">TENSOR</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">여리 블로그</a><p class="is-size-7"><span>&copy; 2021 김덕열</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>