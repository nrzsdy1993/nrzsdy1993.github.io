{"pages":[],"posts":[{"title":"MLP","text":"관련 주소 링크 MLP 모델이란? 퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. 간단히 비교하면, 퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. MLP 모델을 활용한 MNIST 모델 개발 MLP 모델 순서대로 코드를 구현합니다. Step 1. 모듈 불러오기123456import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transforms, datasets Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅123456if torch.cuda.is_available(): DEVICE = torch.device('cuda')else: DEVICE = torch.device('cpu')print(&quot;PyTorch Version:&quot;, torch.__version__, ' Device:', DEVICE) PyTorch Version: 1.9.0+cu102 Device: cuda 12BATCH_SIZE = 32 # 데이터가 32개로 구성되어 있음. EPOCHS = 10 # 전체 데이터 셋을 10번 반복해 학습함. Step 3. 데이터 다운로드 torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다. ToTensor() 활용하여 데이터셋을 tensor 형태로 변환 한 픽셀은 0255 범위의 스칼라 값으로 구성, 이를 01 범위에서 정규화 과정 진행 DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미 Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수 12345678910111213141516train_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = True, download = True, transform = transforms.ToTensor())test_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = False, transform = transforms.ToTensor())train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle=False) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Failed to download (trying next): HTTP Error 503: Service Unavailable Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value=''))) Extracting ../data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/MNIST/raw /usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.) return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) step 4. 데이터 확인 및 시각화 데이터를 확인하고 시각화를 진행합니다. 32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음 1234for (X_train, y_train) in train_loader: print('X_train:', X_train.size(), 'type:', X_train.type()) print('y_train:', y_train.size(), 'type:', y_train.type()) break X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor y_train: torch.Size([32]) type: torch.LongTensor 1234567pltsize = 1plt.figure(figsize=(10 * pltsize, pltsize))for i in range(10): plt.subplot(1, 10, i + 1) plt.axis('off') plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=&quot;gray_r&quot;) plt.title('Class: ' + str(y_train[i].item())) step 5. MLP 모델 설계 torch 모듈을 이용해 MLP를 설계합니다. 12345678910111213141516171819class Net(nn.Module): ''' Forward Propagation 정의 ''' def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28 * 28 * 1, 512) # (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정 self.fc2 = nn.Linear(512, 256) # Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정 self.fc3 = nn.Linear(256, 10) # Input 노드수는 256, Output 노드수는 10개로 지정 def forward(self, x): x = x.view(-1, 28 * 28) # 1차원으로 펼친 이미지 데이터 통과 x = self.fc1(x) x = F.sigmoid(x) x = self.fc2(x) x = F.sigmoid(x) x = self.fc3(x) x = F.log_softmax(x, dim = 1) return x step 6. 옵티마이저 목적 함수 설정 Back Propagation 설정 위한 목적 함수 설정 1234model = Net().to(DEVICE)optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)criterion = nn.CrossEntropyLoss() # output 값과 원-핫 인코딩 값과의 Loss print(model) Net( (fc1): Linear(in_features=784, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=256, bias=True) (fc3): Linear(in_features=256, out_features=10, bias=True) ) step 7. MLP 모델 학습 MLP 모델을 학습 상태로 지정하는 코드를 구현 1234567891011121314def train(model, train_loader, optimizer, log_interval): model.train() for batch_idx, (image, label) in enumerate(train_loader): # 모형 학습 image = image.to(DEVICE) label = label.to(DEVICE) optimizer.zero_grad() # Optimizer의 Gradient 초기화 output = model(image) loss = criterion(output, label) loss.backward() # back propagation 계산 optimizer.step() if batch_idx % log_interval == 0: print(&quot;Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loass: {:.6f}&quot;.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) step 8. 검증 데이터 확인 함수1234567891011121314151617def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(DEVICE) label = label.to(DEVICE) output = model(image) test_loss += criterion(output, label).item() prediction = output.max(1, keepdim = True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= len(test_loader.dataset) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracy 모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어 step 9. MLP 학습 실행1234for Epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, log_interval=200) test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n&quot;.format(Epoch, test_loss, test_accuracy)) /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;) Train Epoch: 1 [0/60000(0%)] Train Loass: 2.343919 Train Epoch: 1 [6400/60000(11%)] Train Loass: 2.304868 Train Epoch: 1 [12800/60000(21%)] Train Loass: 2.276658 Train Epoch: 1 [19200/60000(32%)] Train Loass: 2.320047 Train Epoch: 1 [25600/60000(43%)] Train Loass: 2.309569 Train Epoch: 1 [32000/60000(53%)] Train Loass: 2.293312 Train Epoch: 1 [38400/60000(64%)] Train Loass: 2.264061 Train Epoch: 1 [44800/60000(75%)] Train Loass: 2.311200 Train Epoch: 1 [51200/60000(85%)] Train Loass: 2.261489 Train Epoch: 1 [57600/60000(96%)] Train Loass: 2.260837 [EPOCH: 1], Test Loss: 0.0698, Test Accuracy: 36.43 % Train Epoch: 2 [0/60000(0%)] Train Loass: 2.233048 Train Epoch: 2 [6400/60000(11%)] Train Loass: 2.196863 Train Epoch: 2 [12800/60000(21%)] Train Loass: 2.173305 Train Epoch: 2 [19200/60000(32%)] Train Loass: 2.120425 Train Epoch: 2 [25600/60000(43%)] Train Loass: 2.023674 Train Epoch: 2 [32000/60000(53%)] Train Loass: 1.936951 Train Epoch: 2 [38400/60000(64%)] Train Loass: 1.738194 Train Epoch: 2 [44800/60000(75%)] Train Loass: 1.493235 Train Epoch: 2 [51200/60000(85%)] Train Loass: 1.547187 Train Epoch: 2 [57600/60000(96%)] Train Loass: 1.400939 [EPOCH: 2], Test Loss: 0.0402, Test Accuracy: 60.40 % Train Epoch: 3 [0/60000(0%)] Train Loass: 1.429194 Train Epoch: 3 [6400/60000(11%)] Train Loass: 1.178954 Train Epoch: 3 [12800/60000(21%)] Train Loass: 0.970049 Train Epoch: 3 [19200/60000(32%)] Train Loass: 1.105888 Train Epoch: 3 [25600/60000(43%)] Train Loass: 0.926736 Train Epoch: 3 [32000/60000(53%)] Train Loass: 0.794726 Train Epoch: 3 [38400/60000(64%)] Train Loass: 0.828843 Train Epoch: 3 [44800/60000(75%)] Train Loass: 0.872613 Train Epoch: 3 [51200/60000(85%)] Train Loass: 0.713790 Train Epoch: 3 [57600/60000(96%)] Train Loass: 0.464628 [EPOCH: 3], Test Loss: 0.0243, Test Accuracy: 76.67 % Train Epoch: 4 [0/60000(0%)] Train Loass: 0.507696 Train Epoch: 4 [6400/60000(11%)] Train Loass: 0.635314 Train Epoch: 4 [12800/60000(21%)] Train Loass: 0.636553 Train Epoch: 4 [19200/60000(32%)] Train Loass: 0.875113 Train Epoch: 4 [25600/60000(43%)] Train Loass: 0.558601 Train Epoch: 4 [32000/60000(53%)] Train Loass: 0.462972 Train Epoch: 4 [38400/60000(64%)] Train Loass: 0.630431 Train Epoch: 4 [44800/60000(75%)] Train Loass: 0.428842 Train Epoch: 4 [51200/60000(85%)] Train Loass: 0.826000 Train Epoch: 4 [57600/60000(96%)] Train Loass: 0.508183 [EPOCH: 4], Test Loss: 0.0182, Test Accuracy: 83.32 % Train Epoch: 5 [0/60000(0%)] Train Loass: 0.576632 Train Epoch: 5 [6400/60000(11%)] Train Loass: 0.437238 Train Epoch: 5 [12800/60000(21%)] Train Loass: 0.942570 Train Epoch: 5 [19200/60000(32%)] Train Loass: 0.412996 Train Epoch: 5 [25600/60000(43%)] Train Loass: 0.362784 Train Epoch: 5 [32000/60000(53%)] Train Loass: 0.677201 Train Epoch: 5 [38400/60000(64%)] Train Loass: 0.706629 Train Epoch: 5 [44800/60000(75%)] Train Loass: 0.587188 Train Epoch: 5 [51200/60000(85%)] Train Loass: 0.584579 Train Epoch: 5 [57600/60000(96%)] Train Loass: 0.602702 [EPOCH: 5], Test Loss: 0.0147, Test Accuracy: 86.63 % Train Epoch: 6 [0/60000(0%)] Train Loass: 0.388631 Train Epoch: 6 [6400/60000(11%)] Train Loass: 0.571896 Train Epoch: 6 [12800/60000(21%)] Train Loass: 0.287511 Train Epoch: 6 [19200/60000(32%)] Train Loass: 0.394190 Train Epoch: 6 [25600/60000(43%)] Train Loass: 0.264939 Train Epoch: 6 [32000/60000(53%)] Train Loass: 0.495090 Train Epoch: 6 [38400/60000(64%)] Train Loass: 0.274051 Train Epoch: 6 [44800/60000(75%)] Train Loass: 0.299830 Train Epoch: 6 [51200/60000(85%)] Train Loass: 0.450571 Train Epoch: 6 [57600/60000(96%)] Train Loass: 0.257513 [EPOCH: 6], Test Loss: 0.0129, Test Accuracy: 87.99 % Train Epoch: 7 [0/60000(0%)] Train Loass: 0.377794 Train Epoch: 7 [6400/60000(11%)] Train Loass: 0.630070 Train Epoch: 7 [12800/60000(21%)] Train Loass: 0.320578 Train Epoch: 7 [19200/60000(32%)] Train Loass: 0.658281 Train Epoch: 7 [25600/60000(43%)] Train Loass: 0.643368 Train Epoch: 7 [32000/60000(53%)] Train Loass: 0.420115 Train Epoch: 7 [38400/60000(64%)] Train Loass: 0.687097 Train Epoch: 7 [44800/60000(75%)] Train Loass: 0.430246 Train Epoch: 7 [51200/60000(85%)] Train Loass: 0.343727 Train Epoch: 7 [57600/60000(96%)] Train Loass: 0.577327 [EPOCH: 7], Test Loss: 0.0120, Test Accuracy: 88.78 % Train Epoch: 8 [0/60000(0%)] Train Loass: 0.530810 Train Epoch: 8 [6400/60000(11%)] Train Loass: 0.278406 Train Epoch: 8 [12800/60000(21%)] Train Loass: 0.140812 Train Epoch: 8 [19200/60000(32%)] Train Loass: 0.471123 Train Epoch: 8 [25600/60000(43%)] Train Loass: 0.415495 Train Epoch: 8 [32000/60000(53%)] Train Loass: 0.534980 Train Epoch: 8 [38400/60000(64%)] Train Loass: 0.318894 Train Epoch: 8 [44800/60000(75%)] Train Loass: 0.444783 Train Epoch: 8 [51200/60000(85%)] Train Loass: 0.211995 Train Epoch: 8 [57600/60000(96%)] Train Loass: 0.358874 [EPOCH: 8], Test Loss: 0.0113, Test Accuracy: 89.73 % Train Epoch: 9 [0/60000(0%)] Train Loass: 0.423838 Train Epoch: 9 [6400/60000(11%)] Train Loass: 0.225967 Train Epoch: 9 [12800/60000(21%)] Train Loass: 0.348597 Train Epoch: 9 [19200/60000(32%)] Train Loass: 0.397753 Train Epoch: 9 [25600/60000(43%)] Train Loass: 0.199611 Train Epoch: 9 [32000/60000(53%)] Train Loass: 0.269096 Train Epoch: 9 [38400/60000(64%)] Train Loass: 0.311267 Train Epoch: 9 [44800/60000(75%)] Train Loass: 0.575633 Train Epoch: 9 [51200/60000(85%)] Train Loass: 0.246814 Train Epoch: 9 [57600/60000(96%)] Train Loass: 0.272062 [EPOCH: 9], Test Loss: 0.0108, Test Accuracy: 89.94 % Train Epoch: 10 [0/60000(0%)] Train Loass: 0.399807 Train Epoch: 10 [6400/60000(11%)] Train Loass: 0.282056 Train Epoch: 10 [12800/60000(21%)] Train Loass: 0.270553 Train Epoch: 10 [19200/60000(32%)] Train Loass: 0.605585 Train Epoch: 10 [25600/60000(43%)] Train Loass: 0.333442 Train Epoch: 10 [32000/60000(53%)] Train Loass: 0.395121 Train Epoch: 10 [38400/60000(64%)] Train Loass: 0.676208 Train Epoch: 10 [44800/60000(75%)] Train Loass: 0.170694 Train Epoch: 10 [51200/60000(85%)] Train Loass: 0.160667 Train Epoch: 10 [57600/60000(96%)] Train Loass: 0.381771 [EPOCH: 10], Test Loss: 0.0105, Test Accuracy: 90.28 % train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함. 학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄.","link":"/2021/10/22/step02_2_MLP/"},{"title":"PyTorch","text":"관련 주소 링크 Numpy and PyTorch Tensors, Comparison12345import numpy as np import torchprint(np.__version__)print(torch.__version__) 1.19.5 1.8.1+cu101 Create an array of ones 12print(np.ones(5))print(torch.ones(5)) [1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.]) Create an array of zeros 12print(np.zeros(5))print(torch.zeros(5)) [0. 0. 0. 0. 0.] tensor([0., 0., 0., 0., 0.]) Create a random array 12print(np.random.rand(5))print(torch.rand(5)) [0.64360299 0.39417631 0.79543956 0.39073155 0.85939403] tensor([0.8248, 0.5975, 0.1073, 0.1417, 0.2082]) Create an array from given values 12print(np.array(5))print(torch.tensor(5)) 5 tensor(5) Get an array shape 12temp = np.zeros((2, 5))print(temp.shape) (2, 5) 12temp = torch.zeros((2, 5))print(temp.shape) torch.Size([2, 5]) Tensor Data Type Define a tensor with a default data type 123x = torch.ones(2, 2)print(x)print(x.dtype) tensor([[1., 1.], [1., 1.]]) torch.float32 Specify the data type when defining a tensor 123x = torch.ones(2, 2, dtype = torch.int8)print(x)print(x.dtype) tensor([[1, 1], [1, 1]], dtype=torch.int8) torch.int8 Changing the tensor’s data type12x = torch.ones(1, dtype=torch.uint8)print(x.dtype) torch.uint8 12x = x.type(torch.float)print(x.dtype) torch.float32 Converting tensors into NumPy arrays123x = torch.rand(2, 2)print(x)print(x.dtype) tensor([[0.7788, 0.0314], [0.4117, 0.6491]]) torch.float32 Convert the tensor into a NumPy array 123y = x.numpy()print(y)print(y.dtype) [[0.77875483 0.0314005 ] [0.41166532 0.64910847]] float32 Arithmetic Operation 텐서를 활용하여 사칙 연산을 수행합니다. Scalar 값 하나의 상수값을 의미한다. 12345678import torchscalar_1 = torch.tensor([1.])scalar_2 = torch.tensor([5.])add = scalar_1 + scalar_2torch_add = torch.add(scalar_1, scalar_2)print(add)print(torch_add) tensor([6.]) tensor([6.]) 1234sub = scalar_1 - scalar_2torch_sub = torch.sub(scalar_1, scalar_2)print(sub)print(torch_sub) tensor([-4.]) tensor([-4.]) 1234mul = scalar_1 * scalar_2torch_mul = torch.mul(scalar_1, scalar_2)print(mul)print(torch_mul) tensor([5.]) tensor([5.]) 1234div = scalar_1 / scalar_2torch_div = torch.div(scalar_1, scalar_2)print(div)print(torch_div) tensor([0.2000]) tensor([0.2000]) Vector 하나의 값을 표현할 때 2개 이상의 수치로 표현한 것 사칙 연산의 문법은 Scalar와 동일하게 적용하기 때문에 생략합니다. torch.dot에 대해 설명하면 다음과 같습니다. 공식: (1 * 4) + (2 * 5) + (3 * 6) = 4 + 10 + 18 = 32 123vector_1 = torch.tensor([1., 2., 3.])vector_2 = torch.tensor([4., 5., 6.])torch.dot(vector_1, vector_2) tensor(32.) Matrix 2개 이상의 벡터 값을 통합해 구성된 값 선형 대수의 기본 단위임. 사칙연산도 기존 Scalar와 동일하기 때문에 생략한다. 행렬 곱 연산은 torch.matmul을 활용할 수 있다. 123456789matrix_1 = torch.tensor([[1., 2.], [3., 4.]])matrix_2 = torch.tensor([[5., 6.], [7., 8.]])print(torch.matmul(matrix_1, matrix_2))print(&quot;-- 행렬 곱 연산 --&quot;)print(1 * 5 + 2 * 7)print(1 * 6 + 2 * 8)print(3 * 5 + 4 * 7)print(3 * 6 + 4 * 8) tensor([[19., 22.], [43., 50.]]) -- 행렬 곱 연산 -- 19 22 43 50 Tensor 행렬이 2차원의 배열이라 한다면, 텐서는 2차원 이상의 배열 1234tensor_1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])tensor_2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16.]]])torch.matmul(tensor_1, tensor_2) tensor([[[ 31., 34.], [ 71., 78.]], [[155., 166.], [211., 226.]]]) Converting NumPy arrays into tensors123x = np.zeros((2, 2), dtype=np.float32)print(x)print(x.dtype) [[0. 0.] [0. 0.]] float32 Convert the numpy array into PyTorch tensor 123y = torch.from_numpy(x)print(y)print(y.dtype) tensor([[0., 0.], [0., 0.]]) torch.float32 Moving between devices123x = torch.tensor([1.5, 2])print(x)print(x.device) tensor([1.5000, 2.0000]) cpu CUDA Device12if torch.cuda.is_available(): device = torch.device(&quot;cuda:0&quot;) 123x = x.to(device)print(x)print(x.device) tensor([1.5000, 2.0000], device='cuda:0') cuda:0 1234device = torch.device('cpu')x = x.to(device)print(x)print(x.device) tensor([1.5000, 2.0000]) cpu 123device = torch.device(&quot;cuda:0&quot;)x = torch.ones(2, 2, device=device)print(x) tensor([[1., 1.], [1., 1.]], device='cuda:0')","link":"/2021/10/20/step01_intro_pytorch/"},{"title":"MNIST dataset","text":"관련 주소 링크 torch GPU 설정1234import torchif torch.cuda.is_available(): device = torch.device(&quot;cuda:0&quot;) MNIST 데이터 셋1234from torchvision import datasetsPATH_DATA = &quot;./data&quot;train_data = datasets.MNIST(PATH_DATA, train = True, download=True) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00&lt;?, ?it/s] Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00&lt;?, ?it/s] Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00&lt;?, ?it/s] Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00&lt;?, ?it/s] Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw /usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.) return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) extract the input data and target labels 123X_train, y_train = train_data.data, train_data.targetsprint(X_train.shape)print(y_train.shape) torch.Size([60000, 28, 28]) torch.Size([60000]) Load the MNIST test dataset 1234val_data = datasets.MNIST(PATH_DATA, train = False, download=True)X_val, y_val = val_data.data, val_data.targetsprint(X_val.shape)print(y_val.shape) torch.Size([10000, 28, 28]) torch.Size([10000]) Add a new dimension to the tensors 1234567if len(X_train.shape) == 3: X_train = X_train.unsqueeze(1)print(X_train.shape)if len(X_val.shape) == 3: X_val = X_val.unsqueeze(1)print(X_val.shape) torch.Size([60000, 1, 28, 28]) torch.Size([10000, 1, 28, 28]) Visualization12345from torchvision import utilsimport matplotlib.pyplot as pltimport numpy as np%matplotlib inline 12345678def show(img): # convert tensor to numpy array np_img = img.numpy() # Convert to H * W * C shape np_img_tr = np.transpose(np_img, (1, 2, 0)) plt.imshow(np_img_tr, interpolation='nearest') 1234X_grid = utils.make_grid(X_train[:20], nrow=4, padding=2)print(X_grid.shape)show(X_grid) torch.Size([3, 152, 122]) 1 1","link":"/2021/10/21/step02_1_MNIST/"},{"title":"Hexo Blog","text":"1. 필수 파일 설치 cmd를 실행 시킨다. 1단계: nodejs.org 다운로드 $node -v 2단계: git-scm.com 다운로드 $git --version 3단계: hexo 설치 $npm install -g hexo-cli 2. 깃허브 설정 두개의 깃허브 Repo를 생성한다. 포스트 버전관리 (name: myblog) 포스트 배포용 관리 (name: rain0430.github.io) rain0430 대신에 각자의 username을 입력하면 된다. 이 때, myblog repo를 git clone을 통해 적당한 경로로 내려 받는다. $git –version 3. 블로그 만들기 바탕화면에 myblog 폴더 생성 $hexo init myblog $cd myblog $npm install $npm install hexo-server --save $npm install hexo-deployer-git --save _config.yml 파일 설정 사이트 정보 수정 title : 제목 subtitle: 부제목 description: author: yourname 블로그 URL 정보 설정 url:https://nrzsdy1993.github.io root: / permalink: :year/:month/:day/:title/ permalink_defaults: 깃허브 연동 deploy: type: git repo: https://github.com/nrzsdy1993/nrzsdy1993.github.io.git branch: main 4. 깃허브에 배포하기$hexo generate $hexo server $hexo deploy","link":"/2021/10/19/Hexo_Blog/"},{"title":"GIT","text":"1. GIt 환경설정 git 명령어를 입력 시, 제대로 실행되지 않았다면 환경변수를 추가한다. 윈도우에서 제어판을 실행한 후 시스템 &gt; 고급 시스템 설정 &gt; 고급 &gt; 환경 변수를 작성한다. 시스템 변수 항목에서 Path를 더블클릭하도록 한다. 환경 변수 편집 창에 C:\\Program Files\\Git\\cmd 경로를 추가한다. 영상을 통해서 한번 보도록 한다. 2. git pull 불러들이기 $git pull 3. git add 깃의 동작을 이해햐려면 먼저 워킹 디렉터리 또는 트리라고 한다. 우선 두가지만 기억하면 된다. untracked 상태: 실제 작업파일 및 폴더가 있는데, 이 공간을 자동으로 추적하지 않는다. 즉, 새로 만든 파일은 모두 untracked 상태인 것이다. tracked 상태: Git이 추적을 할 수 있도록 git add 명령어를 사용한다. 깃은 요청받은 파일들만 추적 관리한다. $git add . 4. git commit commit은 크게 HEAD와 스테이지를 결합하여 새로운 객체를 만드는 것과 유사하다. 즉, 변경된 파일의 차이를 깃 저장소에 기록하는 것을 말한다. -그런데, 변경된 파일을 구별하려면 별도의 메시지를 같이 작성해야 하는데, 이를 커밋 메시지라고 해야 한다. $git commit -m &quot;message&quot; 5. git push 저장하기 $git push","link":"/2021/10/18/Git%20(2)/"}],"tags":[{"name":"MLP","slug":"MLP","link":"/tags/MLP/"},{"name":"PYTORCH","slug":"PYTORCH","link":"/tags/PYTORCH/"},{"name":"TENSOR","slug":"TENSOR","link":"/tags/TENSOR/"},{"name":"CUDA_DEVICE","slug":"CUDA-DEVICE","link":"/tags/CUDA-DEVICE/"},{"name":"MNIST","slug":"MNIST","link":"/tags/MNIST/"},{"name":"HEXO","slug":"HEXO","link":"/tags/HEXO/"},{"name":"GIT","slug":"GIT","link":"/tags/GIT/"}],"categories":[{"name":"딥러닝","slug":"딥러닝","link":"/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"Git","slug":"Git","link":"/categories/Git/"}]}